{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "from InferSent.models import InferSent\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier as rfClf\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/runyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embed words using Infersent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 2\n",
    "MODEL_PATH = 'InferSent/encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = infersent.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = 'InferSent/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>tokenSortRatio</th>\n",
       "      <th>tokenSetRatio</th>\n",
       "      <th>wratio</th>\n",
       "      <th>verb_diff</th>\n",
       "      <th>adj_diff</th>\n",
       "      <th>nn_diff</th>\n",
       "      <th>ne_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>67</td>\n",
       "      <td>86</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "   tokenSortRatio  tokenSetRatio  wratio  verb_diff  adj_diff   nn_diff  \\\n",
       "0              93            100      95   0.000000  0.000000  0.090909   \n",
       "1              63             86      86   1.000000  1.000000  0.230769   \n",
       "2              63             63      60   0.500000  0.333333  0.666667   \n",
       "3              24             28      27   0.666667  1.000000  1.000000   \n",
       "4              47             67      86   1.000000  0.333333  0.600000   \n",
       "\n",
       "   ne_diff  \n",
       "0      1.0  \n",
       "1      1.0  \n",
       "2      1.0  \n",
       "3      1.0  \n",
       "4      0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('finaldata.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = data.question1.values\n",
    "q2 = data.question2.values\n",
    "labels = data.is_duplicate.values\n",
    "\n",
    "X = np.stack((q1, q2), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102976(/137513) words with w2v vectors\n",
      "Vocab size : 102976\n"
     ]
    }
   ],
   "source": [
    "questions = list(q1) + list(q2)\n",
    "model.build_vocab(questions, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb words kept : 3254587/3664154 (88.8%)\n",
      "Speed : 1763.9 sentences/s (gpu mode, bsize=64)\n",
      "Nb words kept : 3316153/3729784 (88.9%)\n",
      "Speed : 1608.1 sentences/s (gpu mode, bsize=64)\n",
      "Nb words kept : 1393317/1568232 (88.8%)\n",
      "Speed : 1751.1 sentences/s (gpu mode, bsize=64)\n",
      "Nb words kept : 1421941/1599534 (88.9%)\n",
      "Speed : 1784.6 sentences/s (gpu mode, bsize=64)\n"
     ]
    }
   ],
   "source": [
    "q1_train_embd = model.encode(Q1_train, tokenize=False, verbose=True)\n",
    "q2_train_embd = model.encode(Q2_train, tokenize=False, verbose=True)\n",
    "q1_test_embd = model.encode(Q1_test, tokenize=False, verbose=True)\n",
    "q2_test_embd = model.encode(Q2_test, tokenize=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = abs(q1_train_embd - q2_train_embd)\n",
    "X_test = abs(q1_test_embd - q2_test_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    B =(C/C.sum(axis=0))\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    \n",
    "    labels = [1,2]\n",
    "    # representing A in heatmap format\n",
    "    cmap=sns.light_palette(\"blue\")\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Precision matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # representing B in heatmap format\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.title(\"Recall matrix\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and predict on XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = xgb.XGBClassifier(max_depth=10,learning_rate=0.131131,n_estimators=500,\n",
    "                        min_child_weight=6,\n",
    "                        reg_alpha=119.704012,reg_lambda=115.715236,\n",
    "                        gamma=3.768808,colsample_bytree=0.911753,n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "predict_y = clf.predict_proba(X_test)\n",
    "y_pred =np.argmax(predict_y,axis=1)\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and predict on Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = rfClf(n_estimators=500, max_depth=10, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, time, json\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 254700 samples, validate on 28300 samples\n",
      "Epoch 1/10\n",
      "254700/254700 [==============================] - 40s 157us/step - loss: 0.6143 - acc: 0.7203 - val_loss: 0.5355 - val_acc: 0.7234\n",
      "Epoch 2/10\n",
      "254700/254700 [==============================] - 39s 155us/step - loss: 0.5280 - acc: 0.7357 - val_loss: 0.5197 - val_acc: 0.7409\n",
      "Epoch 3/10\n",
      "254700/254700 [==============================] - 40s 158us/step - loss: 0.5065 - acc: 0.7405 - val_loss: 0.5160 - val_acc: 0.7337\n",
      "Epoch 4/10\n",
      "254700/254700 [==============================] - 40s 155us/step - loss: 0.4982 - acc: 0.7461 - val_loss: 0.4722 - val_acc: 0.7581\n",
      "Epoch 5/10\n",
      "254700/254700 [==============================] - 39s 155us/step - loss: 0.4734 - acc: 0.7581 - val_loss: 0.4719 - val_acc: 0.7555\n",
      "Epoch 6/10\n",
      "254700/254700 [==============================] - 40s 156us/step - loss: 0.4872 - acc: 0.7525 - val_loss: 0.5507 - val_acc: 0.7290\n",
      "Epoch 7/10\n",
      "254700/254700 [==============================] - 40s 156us/step - loss: 0.4829 - acc: 0.7540 - val_loss: 0.5088 - val_acc: 0.7364\n",
      "Epoch 8/10\n",
      "254700/254700 [==============================] - 40s 156us/step - loss: 0.4925 - acc: 0.7449 - val_loss: 0.5507 - val_acc: 0.6993\n",
      "Epoch 9/10\n",
      "254700/254700 [==============================] - 40s 157us/step - loss: 0.5036 - acc: 0.7412 - val_loss: 0.5089 - val_acc: 0.7482\n",
      "Epoch 10/10\n",
      "254700/254700 [==============================] - 40s 158us/step - loss: 0.4882 - acc: 0.7442 - val_loss: 0.6126 - val_acc: 0.6636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a3118ccf8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "MODEL_WEIGHTS_FILE = \"best_weights.h5\"\n",
    "#early_stopping_monitor = EarlyStopping(patience=3)\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "model_mc = Sequential()\n",
    "\n",
    "#add model layers\n",
    "model_mc.add(Dense(200, activation='relu', input_shape=(4096,)))\n",
    "model_mc.add(Dense(200, activation='relu'))\n",
    "model_mc.add(Dense(200, activation='relu'))\n",
    "model_mc.add(Dense(1))\n",
    "\n",
    "model_mc.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#train model\n",
    "model_mc.fit(X_train, y_train, validation_split=0.1, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61371, 15137],\n",
       "       [12730, 32049]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mc.load_weights(MODEL_WEIGHTS_FILE)\n",
    "y_pred = model_mc.predict(X_test)\n",
    "y_pred =(y_pred>0.5)\n",
    "list(y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7157149556711851, 0.6792056966049251, 0.7702391847436246)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred), precision_score(y_test, y_pred), accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7164742401572165"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6886389491081586"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7757220477050302"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
